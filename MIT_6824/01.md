# MapReduce paper

* [link](http://nil.csail.mit.edu/6.824/2020/papers/mapreduce.pdf)

## Intro

* Google processes large amounts of raw data, and this requires distributing the work across many machines in order to finish in a reasonable amount of time
  * simple computations become very complex due to parallelization, distribution of data, handling failures, etc..
* key insight - many computations can be modeled using the `map` and `reduce` primitives present in functional languages
  * `map` records in input to compute intermediate key/val pairs
  * `reduce` all the values that shared the same key

## Programming Model

* `Map: (k1, v1) -> (k2, v2) list`
  * written by the user
  * takes input key/val pair and produces a set of *intermediate* key/val pair
* `Reduce: (k2, v2 list) -> v2 list` 
  * also written by the user
  * accepts intermediate key $I$ and a set of values, then reduces them
* example problem:
  * counting URL access frequency
  * `map` processes input, emits `(URL, 1)`
  * `reduce` is just a sum function, emits `(URL, total_count)`

## Implementation

* implementation choice depends on the environment
  * this paper focuses on Google's environment

### Execution overview

* partition input data into $M$ splits, which can be processed in parallel
* partition intermediate key space into $R$ pieces using a partitioning function (e.g. `hash(key) % R`)
  * $R$ and partitioning function supplied by user

1. splits input files into $M$ pieces, then starts up many copies of the program on a cluster of machines
2. 1 master copy, others are workers. Master picks workers and assigns tasks
3. Worker who is assigned map tasks reads the corresponding input split. Parses key/value pairs, runs `Map` on it, and buffers intermediate pairs
4. periodically, buffered pairs are written to local disk, partitioned into $R$ regions using partition function
   * pass these locations back to master
5. master notifies reduce worker about disk locations of intermediate data. Uses RPC to read buffered data, and sorts by intermediate keys to group them together
   * if too much intermediate data, can use external sort (external merge sort maybe?)
6. reduce worker passes pairs of `(intermediate_key, value list)` to `Reduce`. Output is appended to output file
7. when all tasks are done, master wakes up user program

### Fault Tolerance

* **worker failure**
  * master sends heartbeats to worker
  * if worker is dead, all completed map tasks and any in-progress task by the worker are reset back to their initial idle state
    * available for scheduling on other workers
    * why re-execute completed map tasks? the results are stored on local disk of the machine and thus inaccessible
    * completed reduce tasks are in global file system, that's okay
  * suppose map task is executed by worker $A$ and then $B$ because $A$ failed
    * workers executing reduce tasks are notified
    * any reduce task that has not already read the data from worker $A$ will read instead from $B$
* **master failure**
  * simple to write periodic checkpoints of master data structures
  * if master task, can just start new copy from last checkpointed state

### Practical Considerations

* $M,R$ are constrained because the master must use $O(M \cdot R)$ space for state
  * For each task, it must store the state and identity of the worker machine
  * for each completed map task, it must also store the location/size of the $R$ intermediate files produced
    * needs to send these to the reduce tasks
* $R$ may also be constrained by the user since the output of each reduce task ends up in a separate output file
* They tend to choose $M$ to split the input data into 16-64 MB of data
* Sometimes we get "stragglers" that take unusually long to complete their task
  * e.g. a bad disk, other competing tasks, etc..
  * solution is **backup tasks**
* **backup tasks** - when MapReduce is almost done, the master schedules redundant executions of the in-progress tasks
  * we take the output of whichever one completes first
  * this actually has a significant practical speedup!

## Refinements

* you may want to use a custom partitioning function
  * e.g. `hash(hostname(url))` instead of just `hash(url)` will ensure the same hosts get put in the same file
* recall that the reduce tasks sort the intermediate key/val pairs - this makes it trivial to make a partition's output file sorted
* You can pass in a `Combiner` function that does partial merging before sending it over a network
  * think of this as like an intermediate reduce function that is run on every map task
  * e.g. word counting.. you don't want to send thousands of `('hello', 1)` records. Combining them beforehand and getting fewer `('hello', n_1)` records would be preferable
* bugs in `Map` or `Reduce` can cause deterministic crashing
  * obviously fixing the bug is ideal, but sometimes it doesn't work (e.g. 3rd party lib)
  * there's an optional execution mode that detects deterministic crashes and just skips those records

# Lecture

* reasons for distributed systems
  * **parallelism**
  * **fault tolerance**
  * physical
  * security/isolation
* challenges
  * concurrency
  * partial failures
  * performance
* components of the course
  * lectures
  * papers (for almost every lecture lol)
  * exams
  * labs
  * optional final project
* 